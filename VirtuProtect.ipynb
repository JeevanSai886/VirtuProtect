import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import json

# Configure the Gemini API
genai.configure(api_key="AIzaSyD6r02sNQ2G0-0u7yn1jVNp94gEtA1wFuI")

# Load the fine-tuned model
model = genai.GenerativeModel(model_name="tunedModels/groomingdetectionmodel-8sytka78xdtv")

# Define the refined and detailed prompt for the task
context_prompt = """
You are an AI model trained to supervise and analyze typed data provided from the keyboard of minor girls.
The data may include chat replies, browsing history, social media captions, and comments.
Your task is to carefully analyze the content and identify any patterns of grooming, unsafe messages, 18+ websites, or any form of potentially unsafe or harmful behavior.
NOTE : All the messages you can see in the provided data is typed by the user.
### Your Responsibilities:
1. **Classification**: Classify each input as either:
   - "Safe": If there is no indication of harm or unsafe behavior.
   - "Potentially Unsafe": If there are grooming indicators, risky content, or unsafe patterns.
   - "Dangerous": If the message clearly poses a threat, contains grooming, or includes highly unsafe material.

2. **Risk Scaling**: Assign a risk score based on the following criteria:
   - **1–3**: Minimal risk. The message is mostly safe, but there is a slight possibility of grooming or unsafe behavior.
   - **4–5**: Moderate risk. Some indications of grooming or unsafe behavior, but not fully confirmed.
   - **6–7**: High risk. Grooming patterns, explicit unsafe behavior, or potential threats are strongly suspected.
   - **9–10**: Critical risk. The content is confirmed to be highly unsafe, harmful, or manipulative.

3. **Highlight Suspicious Words**: Extract and list any specific words, phrases, or terms that contribute to your classification and scaling. These words should be highlighted as "Suspicious Words."

4. **Provide Reasoning**: Offer a clear and concise explanation for your classification and risk scaling. The explanation should help identify why the content is classified as it is.

### Expected JSON Response Format:
For each input message, provide the response in this format:
```json
{
  "classification": "Safe / Potentially Unsafe / Dangerous",
  "risk_level": (1–10),
  "suspicious_words": ["word1", "word2", ...],
  "reason": "Detailed explanation for the classification and risk score."
}
"""

# Test input message
input_message = input("Enter your messages :")
prompt = context_prompt + input_message

# Generate the response
response = model.generate_content(
    [prompt],
    safety_settings={
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }
)

# Handle and parse the response
if response.candidates:
    try:
        raw_content = response.candidates[0].content.parts[0].text

        # Attempt to clean and load the JSON response
        cleaned_content = raw_content.replace("\n", "").replace("'", '"')  # Fix JSON formatting issues
        json_output = json.loads(cleaned_content)

        print("Parsed JSON Response:", json_output)

        # Trigger alerts based on the risk level
        if json_output["risk_level"] >= 6:
            print("ALERT: Risky behavior detected! Notify parents.")
    except json.JSONDecodeError:
        print("Failed to decode JSON. Raw Response Content:", raw_content)
else:
    print("Blocked prompt. Reason:", response.prompt_feedback.get('block_reason', 'Unknown'))
