{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "import json\n",
        "\n",
        "# Configure the Gemini API\n",
        "genai.configure(api_key=\"AIzaSyD6r02sNQ2G0-0u7yn1jVNp94gEtA1wFuI\")\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = genai.GenerativeModel(model_name=\"tunedModels/groomingdetectionmodel-8sytka78xdtv\")\n",
        "\n",
        "# Define the refined and detailed prompt for the task\n",
        "context_prompt = \"\"\"\n",
        "You are an AI model trained to supervise and analyze typed data provided from the keyboard of minor girls.\n",
        "The data may include chat replies, browsing history, social media captions, and comments.\n",
        "Your task is to carefully analyze the content and identify any patterns of grooming, unsafe messages, 18+ websites, or any form of potentially unsafe or harmful behavior.\n",
        "NOTE : All the messages you can see in the provided data is typed by the user.\n",
        "### Your Responsibilities:\n",
        "1. **Classification**: Classify each input as either:\n",
        "   - \"Safe\": If there is no indication of harm or unsafe behavior.\n",
        "   - \"Potentially Unsafe\": If there are grooming indicators, risky content, or unsafe patterns.\n",
        "   - \"Dangerous\": If the message clearly poses a threat, contains grooming, or includes highly unsafe material.\n",
        "\n",
        "2. **Risk Scaling**: Assign a risk score based on the following criteria:\n",
        "   - **1–3**: Minimal risk. The message is mostly safe, but there is a slight possibility of grooming or unsafe behavior.\n",
        "   - **4–5**: Moderate risk. Some indications of grooming or unsafe behavior, but not fully confirmed.\n",
        "   - **6–7**: High risk. Grooming patterns, explicit unsafe behavior, or potential threats are strongly suspected.\n",
        "   - **9–10**: Critical risk. The content is confirmed to be highly unsafe, harmful, or manipulative.\n",
        "\n",
        "3. **Highlight Suspicious Words**: Extract and list any specific words, phrases, or terms that contribute to your classification and scaling. These words should be highlighted as \"Suspicious Words.\"\n",
        "\n",
        "4. **Provide Reasoning**: Offer a clear and concise explanation for your classification and risk scaling. The explanation should help identify why the content is classified as it is.\n",
        "\n",
        "### Expected JSON Response Format:\n",
        "For each input message, provide the response in this format:\n",
        "```json\n",
        "{\n",
        "  \"classification\": \"Safe / Potentially Unsafe / Dangerous\",\n",
        "  \"risk_level\": (1–10),\n",
        "  \"suspicious_words\": [\"word1\", \"word2\", ...],\n",
        "  \"reason\": \"Detailed explanation for the classification and risk score.\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Test input message\n",
        "input_message = input(\"Enter your messages :\")\n",
        "prompt = context_prompt + input_message\n",
        "\n",
        "# Generate the response\n",
        "response = model.generate_content(\n",
        "    [prompt],\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Handle and parse the response\n",
        "if response.candidates:\n",
        "    try:\n",
        "        raw_content = response.candidates[0].content.parts[0].text\n",
        "\n",
        "        # Attempt to clean and load the JSON response\n",
        "        cleaned_content = raw_content.replace(\"\\n\", \"\").replace(\"'\", '\"')  # Fix JSON formatting issues\n",
        "        json_output = json.loads(cleaned_content)\n",
        "\n",
        "        print(\"Parsed JSON Response:\", json_output)\n",
        "\n",
        "        # Trigger alerts based on the risk level\n",
        "        if json_output[\"risk_level\"] >= 6:\n",
        "            print(\"ALERT: Risky behavior detected! Notify parents.\")\n",
        "    except json.JSONDecodeError:\n",
        "        print(\"Failed to decode JSON. Raw Response Content:\", raw_content)\n",
        "else:\n",
        "    print(\"Blocked prompt. Reason:\", response.prompt_feedback.get('block_reason', 'Unknown'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "YzzIlr1nSbQN",
        "outputId": "bd701828-4cf0-4dd1-ffa0-bbfec8261047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your messages :I think about you a lot. Do you think about me too?\n",
            "Failed to decode JSON. Raw Response Content: {\n",
            "  \"classification\": \"Potentially Unsafe\",\n",
            "  \"risk_level\": 4,\n",
            "  \"suspicious_flagged: true, \n",
            "  \"reason\": \"User signaling interest and seeking validation.\" \n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5QzYQfPgh0S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
